---
title: "Principal component analysis"
author: "Michael C. Jeziorski"
date: 23 Oct 2024
output: html_document
---

```{r setup, include=FALSE, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(psych)
library(psychTools)
library(factoextra)
```

Principal component analysis (PCA) applies a mathematical transformation to a dataset with the intention of accounting for as much of the variation in the data as possible using a reduced number of dimensions, or principal components.  The first principal component is a dimension that accounts for a maximal portion of the total variation in the dataset, and each successive principal component accounts for a maximal part of the remaining variation while being uncorrelated with (orthogonal to) previous components.  

The goal of PCA is to take variables that are likely correlated with each other and transform them into uncorrelated dimensions.  To accomplish this, each variable is centered (mean made equivalent to 0) and scaled (variance made equivalent to 1) so that each is weighted the same.  Matrix algebra is then applied to identify the principal components.  If the dataset has n variables, then n principal components will be generated.  However, the benefit of PCA is that, because each component in turn is accounting for a maximal fraction of the remaining variance, the first L components can explain much more variation in the data than any L of the original variables do.  

The source of the data we will use is the Wisconsin Breast Cancer dataset from the UCI Machine Learning repository.  The following code chunk will read in the data and assign the correct column names.  

```{r, message=FALSE}
# using the Wisconsin Breast Cancer dataset from the UCI Machine learning repository
wdbc <- read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",
                 col_names = FALSE, show_col_types = FALSE)

# The file comes without colume names, we will use features and qualifications.
features <- c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity", 
              "concave_points", "symmetry", "fractal_dimension")
names(wdbc) <- c("id", "diagnosis", paste0(features,"_mean"), paste0(features,"_se"),
                 paste0(features,"_worst"))

```

We can use one of the overview functions like the tidyverse function `glimpse()` to view the structure of the data.  

```{r}
# The data is used at Mike's PCA example.
# We will try the factor analysis subroutines with psych package

# Now for selection only the data without the ID and group
wdbc_all <- wdbc %>% select(-1, -2)
wdbc_mean <- wdbc_all %>% select(radius_mean:fractal_dimension_mean)
```

The dataset contains the mean, standard error (se), and maximum value (worst) for each of ten separate measures of breast tumor morphology.  Although we appear to have 30 independent variables, in fact many of the variables are going to be highly correlated.  We can get an overall idea using the `plot()` function in base R.  

```{r}
wdbc_scale <- scale(wdbc_all)

# First look at the variables in a correlation matrix
# wdbc_mean %>%
#  select(radius_mean:fractal_dimension_mean) %>%
#  plot()
pairs.panels(wdbc_mean, pch = 21, stars = TRUE,  smoother = TRUE)

```

It is no surprise that some of the variables are nearly perfectly correlated, like radius, perimeter, and area, all measures of size.  To see how PCA works, we will perform a PCA on only two variables, choosing two that are not as strongly correlated, `perimeter_mean` and `symmetry_mean`.  

```{r}
wdbc %>%
  ggplot(aes(x = perimeter_mean, y = symmetry_mean)) +
  geom_point(pch = 1) +
  geom_smooth(method = "lm", color = "blue", se = FALSE)
# starting with latent variable exploratory factor analysis (EFA)
# the PCS for all
# principal(wdbc_all, 30, rotate = "none", scores = TRUE, cor = "cor", method = "regression")

pca_wdbc <- principal(wdbc_scale, 30, rotate = "varimax", scores = TRUE, use ="pairwise", cor = "cor", covar = TRUE, method = "regression")
plot(pca_wdbc$values, type = "b", ylab = "Eigenvalues", xlab = "Component Number")

scores_pca_wdbc <- pca_wdbc$scores
text(scores_pca_wdbc[,1], scores_pca_wdbc[,2], labels=rownames(scores_pca_wdbc), cex=0.7)
```

Both `principal()` and `fa()` are functions from the `psych` package that generate a principal component analysis and a factor analisis on the data; we will use `principal()` with the subset of the dataframe. We will choose four factors.  A summary of the results can be viewed using `summary()`.  

```{r}
pca_wdbc_4c <- principal(wdbc_scale, 4, rotate = "varimax", scores = TRUE, cor = "cor", method = "regression")
biplot(pca_wdbc_4c)
pca_wdbc_4c
summary(pca_wdbc_4c)

iclust(wdbc_scale, nclusters = 4)
# 
fa_wdbc_4c <- fa(wdbc_scale, 4, rotate="varimax", fm="ml", oblique.scores=TRUE)
fa_wdbc_4c

pca_wdbc_4c_loadings_df <- as_tibble(pca_wdbc_4c$loadings[])
pca_wdbc_4c_loadings_df$variable <- rownames(pca_wdbc_4c_loadings_df)
pca_wdbc_4c_loadings_long <- pivot_longer(pca_wdbc_4c_loadings_df, -variable, names_to = "component", values_to = "loading")
# boxplot(loading~component, data=pca_wdbc_4c_loadings_long)

ggplot(pca_wdbc_4c_loadings_long, aes(x = component, y = loading)) +
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Principal Component Loadings", x = "Component", y = "Loading")



```
The object created by `fa()` is a list containing vectors and matrices.  The main factos components generated can be extracted by subsetting the `x` element.  These are the same data points as before, but now centered, scaled, and rotated.