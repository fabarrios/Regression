---
title: "Logistic Regression"
author: "F.A. Barrios<br><small>UNAM<br></small>"
date: "`r Sys.Date()`"
output:
  rmdformats::robobook:
    highlight: kate
---

```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(cache = TRUE)
```

```{r, echo=FALSE }
library(tidyverse)
library(car)
library(effects)
```

# Introduction.    

"Odds: the chances of something happening"  
 Odds provide a measure of the likelihood of a particular outcome.  
 
*odds of an outcome   - ratio of the number of events that produce the outcome to  
                      to the number that do not.*  
                      
The odds are calculated as the ratio of the probability of the event occurring to the probability of it not occurring:  
$$ Odds = \frac {p}{1âˆ’p} $$
The binary logistic regression model studies how a set of predictor variables $X$ is related to a dichotomous response variable $Y$.  We will define the response to be $Y = 0$ or $1$, with $Y = 1$ denoting the occurrence of the event of interest.  
A linear attempt to model a vector $X$ of predictors $\{ X_1, X_2, \dots , X_k \}$ will look like:  
$$ E[Y|X] = X\beta$$  
since the expectation $E[Y|X]$ of a binary variable $Y$ is $Prob\{Y=1|X\}$. A purely linear model like this cannot easily fit the data over the whole range.  Therefore the statistically preferred model for the analysis of binary responses is instead the binary logistic regression model, stated in terms of the probability that $Y=1$ given $X$:  
$$ Prob\{Y=1 | X\} = [1+ \exp(-X\beta)]^{-1}$$.  
with $X\beta$ equal to $\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k$.  
  
The function  
$$ p = [1 + \exp(-x)]^{-1} $$  
is called the logistic function, has unlimited range for $x$ while $p$ is restricted to range from $0$ to $1$. Using $1 - p = \exp(-x)/[1 + \exp(-x)]$ we can solve the equation of the logistic for $x$ which is the inverse of the logistic function:  
$$ x = \ln[\frac{p}{1-p}] = \ln[odds \space that \space Y=1 \space occurs] = logit\{Y=1\}$$  
Therefore the logistic model is a direct probability model since it is stated in terms of $Prob[Y=1 | X]$.  

```{r}
# Logistic function
# plot of the logistic function

f_logistic <- function(x) {
  return (1/(1 + exp(-x)))
  }
  
curve(f_logistic,
  from = -12, 
  to = 12,
  ylab = "P",
  xlab = "X",
  main = "Logistic Function")
```

## Model Assumptions  
Since the distribution of a binary random variable $Y$ is defined by the true probability that $Y=1$ and since the model makes no assumption about the distribution of the predictors, the logistic model makes no assumption about distributions. Its only assumptions relate to the form of the regression equation, and are easily understood if transforming $Prob\{Y=1\}$ to a model linear in $X\beta$:  

$$ \tt logit\{Y=1 | X\} = logit(P) = \log[\frac{P}{1-P}] = X \beta$$  

where *P = Prob{Y = 1 | X}* the model is a linear regression model in the log odds that *Y = 1* since *logit(P)* is a weighted sum of the *Xs*,  

$$ \tt logit\{Y=1|X\} = \beta_0 + \beta_1 X_1 + \dots + \beta_j X_j + \dots + \beta_k X_k$$
$$ = \beta_j X_j + C$$
where all other factors are held constant, $C$ is a constant given by  

$$ C = \beta_0 + \beta_1 X_1 + \dots + \beta_{j-1}X_{j-1} + \beta_{j+1} X_{j+1} + \dots + \beta_k X_k $$  

Therefore the parameter $\beta_j$ is the change in the *log odds* per unit change in $X_j$ if $X_j$ represents a single factor that is linear and does not interact with other factors and if all the other factors are held constant. We can write the result in terms or the *odds that Y = 1*:
$$ \tt odds\{Y=1 | X\} = exp(X\beta) $$
and if all factors other than $X_j$ are held constant,

$$ \tt odds\{Y=1 | X\} = \exp(B_j X_j + C) = \exp(\beta_j X_j) \exp(C) $$
The regression parameters can also be written in terms of *odds ratios*. The odds that $Y = 1$ when $X_j$ is increased by $d$, divided by the odds at $X_j$ is:  
$$ \frac{ odds\{Y=1 | X_1, \dots, X_j+d, \dots, X_k\}}{odds\{Y=1 | X_1, \dots, X_j,\dots, X_k \}} $$  
$$ = \frac{\exp[\beta_j(X_j+d)]\exp(C)}{\exp[\beta_j X_j]\exp(C)} $$  
$$ = \exp[\beta_j X_j + \beta_j d - \beta_j X_j] = \exp(\beta_j d)$$
The effect of increasing $X_j$ by $d$ is to increase the odds that $Y = 1$ by a factor of $\exp(\beta_j d)$, or to increase the *log odds* that *Y = 1* by an increment of $\beta_j d$.  

## Interpretation of Regression Coefficients  
```{r}
wcgs <- read_csv("https://raw.githubusercontent.com/fabarrios/Regression/master/DataRegressBook/Chap2/wcgs.csv",
        show_col_types = FALSE)
```

To estimate a logistic regression model the general formula in the predictor variables is given by :    

$$log[\frac{\hat\mu(X)}{1-\hat\mu(X)}] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_k X_k$$

If we exponentiate both sides of the equation, we get  

$$\frac{\hat\mu(X)}{1-\hat\mu(X)} = exp(\beta_0) \times exp(\beta_1 X_1) \times exp(\beta_2 X_2) \times \cdots \times exp(\beta_k X_k)$$  

where the left hand of the equation, $\frac{\hat\mu(x)}{1-\hat\mu(x)}$, gives the *fitted odds* of success, **the fitted probability of success divided by the fitted probability of failure**. Exponentiating the model removes the logarithms and changes the model in the log-odds scale to one that is multiplicative, in this log odds scale.  

## Example of Cotollary Heart Disease  

For the WCGS data and the variable Corollary Heart Disease (CHD) and age, the $\beta_1$ is the age slope of the fitted logistic model. The outcome of the model is the log odds of Corollary Heart Disease risk and the relationship with age, the slope coefficient $\beta_1$ gives the change in the log odds of chd69 associated with the model, chd69 is the presence of a cardio-vascular event (1).  

```{r}
wcgs <- mutate(wcgs, chd69 = factor(chd69))
wcgs <- mutate(wcgs, arcus = factor(arcus))
wcgs_n <- wcgs %>% filter(chol < 645)

# For table 5.2
CHD_glm01 <- glm(chd69 ~ age, family = binomial(link = logit), data = wcgs)
S(CHD_glm01)

# confint(CHD_glm01, parm = "age")
# To estimate the model
exp(coef(CHD_glm01)["age"])
```

Wich is identical to the last line in the `summary (S)` table, the exponential transformed $\beta_1$ coefficient (the link-function).  This is the **log-odds** of the *CHD (Corollary heart disease) risk*, the slope coefficient $\beta_1$ gives the change in the log-odds of `chd69` associated with one year increase in `age`.  This odds ration indicate a small (7.7%, change 100x(1.077 - 1.0) -> 0.077 x 100 %) but statistically significant increase in the chances of CHD for each one year age increase, for example for a 10 year increase in odds ratio (d=10) $\exp(\beta_{age} d)$  

$$ \exp(0.074 \times 10) = 2.105 $$

where we used the "link transformation" that is the exponentiation, to obtain the odds of *Y* (logit). In the `effects` library there is a general plotting function for plotting all the predictor effect plots used in a model.  

```{r}
plot(predictorEffects(CHD_glm01))
```

## Multiple predictors example  
We can estimate a linear regression of multiple predictors as the function of the logistic-regression model of the fitted odds of success (fitted probability of the success over the fitted probability of no success).  
In the following model we estimate the chance of the cardio-vascular event `chd69` now as response of the `age`, `chol`, `bmi`, `sbp` predictors and the binary `smoke` predictor (yes, no). We mentioned that logistic regression may not use too many predictors in a linear model since it gets complicated, we will present a couple of examples of multipredictor linear model since it is important.  In addition we will use the `car`and `effects` packages functions and comment their use.

```{r}
# same response variable with multipredictor
# wcgs data with the less than 645 in chol for a outlier with this chol value.
CHD_glm02 <- glm(chd69 ~ age + chol + bmi + sbp + smoke,
                 family = binomial(link = logit), data = wcgs_n)
S(CHD_glm02)

# And the effects plot
plot(predictorEffects(CHD_glm02))
```

## Women's Labor Force Participation example  

Using data from the 1976 U.S. Panel Study of Income Dynamics in which the response (*Y*) variable is married women's labor force participation. The data was used by Mroz (1987) and Berndt(1991) as an exercise in logistic regression.  The data is in the **carData** package in the `Mroz` data frame.  

The variables in the Mroz data frame are:  

 | Variable         | Description     | Remarks |
|--------------|-----------|------------|
| `lfp` | wife's labor force participation  | factor: no, yes     |
| `k5`  | number of children ages 5 and younger  | 0-3, few 3s    |
| `k618`| number of children ages 6 to 18  | 0-8, few > 5    |
| `age` | wife's age in years  | 30-60 |
| `wc`  | wife's college attendance  | factor: no, yes |
| `hc`  | husband's college attendance  | factor: no, yes |
| `lwg` | log of wife's estimated wage rate  | Imputation of wages  | 
| `inc` | family income excluding wife's income | $1000s |  


The variable `lfp` is a factor with two levels, and if we use this variable as the response variable, the `no` level correspond to failure (zero) and `yes`, to success. Applying the `S()` function to a logistic-regression model produces results similar results for a linear (`lm`) model, by default we get additional table of exponentiated coefficients and their confidence limits.  

```{r}
mroz.mod <- glm(lfp ~ k5 + k618 + age + wc + hc + lwg + inc,
                family = binomial(link = logit), data = Mroz)
summary(mroz.mod)
S(mroz.mod)

plot(predictorEffects(mroz.mod))

plot(predictorEffects(mroz.mod), 1)
plot(predictorEffects(mroz.mod), 3)
plot(predictorEffects(mroz.mod), 7)

plot(predictorEffects(mroz.mod), 4)

```

The Wald test for the regression coefficients, given by the *ratios of the coefficient estimates to their standard errors*, are labeled a `z values`.  The predictors `k5`, `age`, `wc`, `lwg`, and `inc` have estimated coefficients associated with very small p-values, while the coefficients for `k618` and `hc`, are associated with large p-values.


## Model Testing in Logistic Regression  

We have used the `anova()` function to test linear models, which can also be used to test GLMs that differ by one or more terms.  To test the hypothesis that women labor force participation does not depend on the number of young children, the number of older children or both  We can remove the `k5` and `k618` variables from the Mroz model. And compare with the "full model" using the R ANOVA function.

```{r}
mroz_02.mod <- glm(lfp ~ age + wc + hc + lwg + inc,
                family = binomial(link = logit), data = Mroz)
S(mroz_02.mod)

anova(mroz.mod, mroz_02.mod, test = "Chisq")
```

The likelihood-ratio test statistic is the chance in deviance between the two fitted models, and the p-value for the test is estimated by comparing the test statistic to the $\chi^2$ distribution with $df$ equal to the change in degrees of freedom for the two models.  In the Mroz example the deviance difference is 66.5 of 2 *df*, that reflect the two regressors (k5 and k618) removed. Then the p-value is effectively zero. Than the probability of a woman's participation in the labor force depends on the number of children she has. Meaning that the inclusion of the predictors k5 and k618, result in a different model. This test is based in the deviance therefore is called an *analysis of deviance*.

The Anova function from the `car` package, estimates the Type-II tests, and for a GLM estimates a likelihood-ratio test that is very similar to the Wald test (in the S() output of the model).  

With the `linearHypothesis` function we can test if the coefficients of `k5` and  `k618` are zero, or if the coefficients `k5` and `k618` are equal.  

```{r}
Anova(mroz.mod)
# To test if the coefficients are zero
linearHypothesis(mroz.mod, c("k5", "k618"))
# To test if the coefficientes are equal
linearHypothesis(mroz.mod, "k5 = k618")
```
