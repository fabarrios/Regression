---
title: "Maximum Likelihood Estimation"
author: "F.A. Barrios"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
  pdf_document:
description: "to prepare Class2020 presentations"
---

```{r setup, include=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

```{r}
# All the needed libraries
library(tidyverse)
library(effects)
```
# Maximum Likelihood Estimation (MLE) 

Que es la Estimación por Máxima Posibilidad --Maximum Likelihood Estimation (MLE)-- ? (en este comentario vamos a usar likelihood --probabilidad, posibilidad-- como posibilidad). Este comentario surgió del comentario en R-Loggers y de la entrada en el blog llamada "Maximum likelihood distilled" [^1]

[^1]: Entrada en el blog the Jorge Cimentada (https://cimentadaj.github.io/blog/2020-11-26-maximum-likelihood-distilled/maximum-likelihood-distilled/)

Como muchas ideas padres, esta la copié del blog de Jorge Cimentada, y es una explicación muy fácil de seguir para entender que el "Maximum Likelihood Estimation", como lo dije al inicio vamos a llamar a "likelihood" la posibilidad, y la idea es calcular valores "posibles" de una funcion y correr un optimizador en varias iterasiones para obtener el valor más probable de dicha función. 

Primero se define una función llamada loglikelihood (la posibilidad logarítmica), dicha funciona tiene el valor inicial de los parámetros a optimizar, y estima un valor probable de el predictor haciendo una llamada a la distribución normal de densidad probable (`dnorm` de R, regresa la "altura" de la campana en el valor x).

```{r}
loglikelihood <- function(parameters, predictor, outcome) {
  # intercept
  a <- parameters[1]
  # beta coef
  b <- parameters[2]
  # error term
  sigma <- parameters[3]

  # Calculate the likelihood of `y` given `a + b * x`
  ll.vec <- dnorm(outcome, a + b * predictor, sigma, log = TRUE)

  # sum that likelihood over all the values in the data
  sum(ll.vec)
}

# Generate three random values for intercept, beta and error term
inits <- runif(3)

# Calculate the likelihood given these three values
loglikelihood(
  inits,
  predictor = mtcars$disp,
  outcome = mtcars$mpg
)

mle <-
  optim(
    inits, # The three random values for intercept, beta and sigma
    loglikelihood, # The loglik function
    lower = c(-Inf, -Inf, 1.e-5), # The lower bound for the three values (all can be negative except sigma, which is 1.e-5)
    method = "L-BFGS-B",
    control = list(fnscale = -1), # This signals to search for the maximum rather than the minimum
    predictor = mtcars$disp,
    outcome = mtcars$mpg
  )


mle$par[1:2]
```


```{r}
# to compare the output from the lm (minimum squares)
coef(lm(mpg ~ disp, data = mtcars))
```

La función `loglikelihood` estima que tan compatible es el valor de Y con la estimación probable de la respuesta lineal `a + b * x`, (en logaritmo), y después esta función usa el optimizador de R para estimar los valores que optimizan a la función

Del paquete original de `stats` de R se tiene una función para optimizar:

`optim(par, fn, gr = NULL, ..., `  
`     method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"), `  
`      lower = -Inf, upper = Inf, `  
`      control = list(), hessian = FALSE) `  

y la función para estimar si se decide hacer un SANN para no volver a estimar el modelo.

`optimHess(par, fn, gr = NULL, ..., control = list()) `



