---
title: "Maximum Likelihood Estimation Introduction"
author: "F.A. Barrios"
date: '2022-05-13'
output:
  rmdformats::robobook:
    highlight: kate
---

```{r setup, include=FALSE}
library(knitr)
library(rmdformats)
# knitr::opts_chunk$set(echo = TRUE)
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               fig_width=8,
               fig_height=6)
```

```{r, echo= FALSE}
library(vcd)
library(tidyverse)
```
## Poisson distribution and examples  

The Poisson distribution with a rate parameter $\lambda = np$ is defined as :
$$
P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}
$$
and in R we can estimate it by using `dpois()`, with $\lambda$ the average.  

## A generative model for epitope detection[^1]

When testing certain pharmaceutical compounds, it is important to detect proteins that provoke an allergic reaction. The molecular sites that are responsible for such reactions are called epitopes. An antibody is a type of protein made by certain white blood cells in response to a foreign substance in the body, which is called the antigen.  
An antibody binds to its antigen. The purpose of the binding is to help destroy the antigen. Antibodies can work in several ways, depending on the nature of the antigen. Some antibodies destroy antigens directly. Others help recruit white blood cells to destroy the antigen. An epitope, also known as antigenic determinant, is the part of an antigen that is recognized by the immune system, specifically by antibodies, B cells or T cells.  
ELISA assays are used to detect specific epitopes at different positions along a protein. Suppose the following facts hold for an ELISA array we are using:  

1. The baseline noise level per position, or more precisely the false positive rate, is 1%. This is the probability of declaring a hit – we think we have an epitope – when there is none. 
2. The protein is tested at 100 different positions, supposed to be independent.  
3. We are going to examine a collection of 50 patient samples.

We’re going to study the data for all 50 patients tallied at each of the 100 positions. If there are no allergic reactions, the false positive rate means that for one patient, each individual position has a probability of 1 in 100 of being a 1. So, after tallying 50 patients, we expect at any given position the sum of the 50 observed (0,1) variables to have a Poisson distribution with parameter 0.5.  

[^1]: Notes taken from the Holmes S. & Huber W, Modern Statistics for Modern Biology, Cambridge, 2019.

```{r}
load("~/Dropbox/GitHub/Regression/DataMSforMB/e100.Rdata")

e100
barplot(table(e100), space = 0.05, col = "chartreuse3")
```

## Estimating the parameter of the Poisson distribution  

Can we try out different values of the parameter $\lambda$ and see which gives the best fit to our data, as a "manual" guessing process.

```{r}
table(e100)
# A Poisson distribution for parameter 3, to generate a 100 numbers
table(rpois(100, 3))
table(rpois(100, 2))
table(rpois(100, .8))
table(rpois(100, .3))
```
**Maximum Likelihood Estimation (MLE)** is a statistical method used to estimate the parameters of a probability distribution model from observed data. The key idea is to find the parameter values that maximize the likelihood of obtaining the observed data under the assumed probability model.

We can estimate which parameter value maximizes our data. The parameter that maximizes our data can be estimated calculating the probability of seeing the data if the value of the Poisson parameter is *m*. Since de data comes from independent draws, the probability is the product of individual probabilities:  
$$
P({\verb+ 58 zeroe, 34 ones, 7 twos, 1 seven+} \space | \space {\verb+data are Poisson(m)+}) \\
= P(0)^{58} \times P(1)^{34} \times P(2)^7 \times P(7)^1
$$
```{r}
prod(dpois(c(0, 1, 2, 7), lambda = 3)^(c(58, 34, 7, 1)))
prod(dpois(c(0, 1, 2, 7), lambda = 1.5)^(c(58, 34, 7, 1)))
prod(dpois(c(0, 1, 2, 7), lambda = .8)^(c(58, 34, 7, 1)))
```

This probability as a function of the parameter *m* is the *likelihood function* of the parameter $\lambda$ and the value that maximizes it, is the maximum likelihood value. This likelihood function can be written as:
$$
L(\lambda, x=(k_1, k_2, k_3, \dots)) = \prod_{i=1}^{100}f(k_i)
$$
is mush easier work with logarithms, and avoid use multiplication of very small numbers therefore we work with the log of the likelihood function. Since the logarithm is strictly an increasing function, if there is a point where it reaches a maximum the probability will be maximum in that point.

We can estimate the likelihood for different poisson parameters.

```{r}
loglikelihood <- function(lambda, data = e100) {
  sum(log(dpois(data, lambda)))
}
```

And we can compute the likelihood for any series of parameters lambda needed  

```{r}
lambdas <- seq(0.05, 0.95, length = 100)

loglike <- vapply(lambdas, loglikelihood, numeric(1))
plot(lambdas, loglike, type = "l", col = "red", ylab = "", lwd = 2,
     xlab = expression(lambda))

m0 <- mean(e100)
abline(v = m0, col = "blue", lwd = 2)
abline(h = loglikelihood(m0), col = "purple", lwd = 2)

m0
table(rpois(100, m0))
```

With the function `goodfit` from the `vcd` R package we can estimate the probability distribution in general for experimental data.

```{r}
gf <- goodfit(e100, "poisson")
names(gf)

gf$par
```

The list produced by `goodfit` have the result of the data fitted to the type of distribution selected.  In particular the `par` variable has the adjusted frequency distribution parameter $\lambda$ for the Poisson case.  

There is a formal estimation for finding that the mean maximizes the (log-)likelihood  

$$
log L(\lambda, x) = \sum_{i=1}^{100} (-\lambda + k_i log\lambda - log(k_i!) )\\
                  = -100\lambda + log\lambda(\sum_{i=1}^{100}k_i) + const.
$$
The *const* term is a constant that does not depend on $\lambda$. To find the parameter $\lambda$ that maximizes this, we compute the derivative in $\lambda$ and set it to zero:
$$
\frac{d}{d\lambda}log L = -100 + \frac{1}{\lambda}\sum_{i=1}^{100} k_i = 0 \\
                     \lambda = \frac{1}{100}\sum_{i=1}^{100}k_i \\
                     \lambda = \bar{k}
$$

This is a statistical approach, starting from the data to infer the model parameter(s): this is statistical estimation of a parameter from data.  
There is a very good page by J. Bryer with a visual explanation to understand further MLE [here](https://htmlpreview.github.io/?https://github.com/jbryer/visualMLE/blob/master/doc/mle.html)


## Binomial distribution and maximum likelihood  

The binomial distribution has two parameters: the number of trials *n*, which is typically known, and the probability *p* of seeing a 1 in a trial. This probability is often unknown.  
Suppose we take a sample of n = 120 males and test them for red-green color blindness. We can code the data as 0 if the subject in not color blind and 1 if he is.

```{r}
cb <- c(rep(0,110), rep(1,10))

table(cb)
```

Where the most likely value of *p* for this data is $\hat{p} = \frac{1}{12}$ which is the maximum likelihood estimate. (the hat is for because this is an estimate of *p* made from the data).   

### The binomial distribution  

Mathematical theory tells us that for X distributed as a binomial distribution with parameters *(n, p)*, written $X \sim B(n,p)$ the probability of seeing $X=k$ successes is
$$
P(X=k) = \left( \begin{array}{c} 
          n \\ k
          \end{array} \right) p^k(1-p)^{n-k}
$$
For our problem we will like to estimate the possible distribution of the color blindness table: `cb` we can plot the probability using the `dbinom` function fo R.

For $n=120$ and *10* successes $\hat p = \frac{1}{12}$.  As  

$$
\left( \begin{array}{c} 
          n \\ k
          \end{array} \right)
$$ 
for this problem is very large, $\sim 1.2\times 10^{14}$ using the Stirling's formula it results in around $e^{30}$ we use the logarithm of the likelihood to give

$$
log P(X | n, k) = log( \left( \begin{array}{c} 
          n \\ k
          \end{array} \right) p^k (1-p)^{(n-k)} ) \\
$$

Using our numbers for this fomula we can estimate the funciton of the log likelihood.  

$$
log f(p|k) = 30 + 10log(p) + (120 - 10)log(1-p)          
$$

We can plot the function to visualize the maximum of the function  


```{r}
loglikelihood <- function(theta, n = 120, k = 10) {
  30 + k * log(theta) + (n-k) * log(1-theta)
}

thetas <- seq(0, 1, by = 0.001)
plot(thetas, loglikelihood(thetas), xlab = expression(theta),
     ylab = expression(paste("log f(", theta, " | k)")), type = "l")
abline(v = 1/12, col = "red")
abline(h = loglikelihood(1/12), col = "purple", lwd = 2)
```
