---
title: "General Linear Model"
author: "F.A. Barrios<br><small>Instituto de Neurobiología, UNAM<br></small>"
date: "<small>`r Sys.Date()`</small>"
output:
  rmdformats::robobook:
    highlight: kate
  pdf_document: default
description: "to prepare Class2020 presentations"
---


```{r setup, cache=FALSE, echo=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="80")
opts_chunk$set(echo=TRUE,
               cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=80)
```

```{r}
library(tidyverse)
library(here)
library(wesanderson)
library(multcomp)
library(emmeans)

```

## Linear Regression  

The term "regression" was introduced by Francis Galton (Darwin's nephew) during the XIX century to describe a biological phenomenon.  The heights of the descendants of tall ancestors have the tendency to "return", come back, to the normal average high in the population, known as the regression to the media. (Mr. Galton was an Eugenics supporter)  

## The Least-Square linear regression  

The general equation for the straight line is $y = mx + b_0$, this form is the "slope, intersection form". The slope is the rate of change the gives the change in $y$ for a unit change in $x$. Remember that the slope formula for two pair of points $(x_1, y_1)$ and $(x_2, y_2)$ is:  
$$ m = \frac{(y_2 - y_1)}{(x_2 - x_1)} $$  
Using the expression for all the lines $y_i = \beta_{0_i} + \beta_{1_i}x_i$ for a set of points $(x_i, y_i)$, finding the minimum of the addition of all the differences with the ideal line we estimate the expression for the "best" slope $\hat{\beta_{1}}$ and the independent term $\hat{\beta_{0}}$:  

$$\hat{\beta_{1}} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i -\bar{y} )}{\sum_{i=1}^{n}(x_i - \bar{x})^2} $$  
$$\hat{\beta_{0}} = \bar{y} - \hat{\beta_{1}} \bar x $$  

The basic properties we know about one variable linear regression are:  
  The correlation measures the strength of the relationship between x and y (see this shiny app for an excellent visual overview of correlations).  
  The correlation ranges between -1 and 1.  
  The mean of x and y must fall on the line.  
  The slope of the line is defined as the change in $y$ over the change in $x$; $m= \frac{\Delta y}{\Delta x}$.  
For regression use the ratio of the standard deviations such that the correlation is defined as $m=r\frac{s_y}{s_x}$ where $m$ is the slope, $r$ is the correlation and $\bar{x}$ and $\bar{y}$ the mean, and $s$ is the sample standard deviation.  


## Example of linear regression  

**From the chap 9 Daniel** Després et al. pointed out that the topography of adipose tissue (AT) is associated with metabolic complications considered as risk factors for cardiovascular disease. It is important, they stated, to measure the amount of intraabdominal AT as part of the evaluation of the cardiovascular disease risk of an individual's Computed Tomography (CT), the only available technique that precisely and reliably measures the amount of deep abdominal (AT), however, is costly and requires irradiation of the subject. In addition, the technique is not available to many physicians. Després et al. conducted a study to develop equations to predict the amount of deep abdominal AT from simple anthropometric measurements. Their subjects were men between the ages of 18 and 42 years who were free from metabolic disease that would require treatment. Among the measurements taken on each subject were deep abdominal AT obtained by CT and waist circumference as shown in EXA_C09_S03_01.csv. A question of interest is how well one can predict and estimate deep abdominal AT from knowledge of the waist circumference. This question is typical of those that can be answered by means of regression analysis. Since deep abdominal AT is the variable about which we wish to make predictions and estimations, it is the dependent variable. The variable waist measurement, knowledge of which will be used to make the predictions and estimations, is the independent variable.  

```{r}
# load the data file
Exa9.3 = read_csv("~/Dropbox/GitHub/Regression/DataSets/ch09_all/EXA_C09_S03_01.csv",
         show_col_types = FALSE)
         
names(Exa9.3)

plot(Y ~ X, data = Exa9.3, pch = 20)

# horizontal line and vertical line
Ybar=mean(Exa9.3$Y)
Xbar=mean(Exa9.3$X)

abline(h=Ybar, col = 2, lty = 2)
abline(v=Xbar, col = 2, lty = 2)

# simple linear model
Lin9.3 = lm(Y ~ X, data=Exa9.3)
summary(Lin9.3)

# linear model plot
abline(Lin9.3, col=2)
```
Response variable $Y$ is a random variable that is measured and has a distribution with expected value $E(Y|x)$ given a set of independent variables $x$.  
$$Y_j (j=1, . . . , J)$$  
for a set of $x_{jl}$ predictor variables (or independent variables) defined as vectors for each $j$  
$$ x_{jl} (l=1, . . . , L)$$  
with $L(L<J)$, a general linear model with an error function $\epsilon_j$ can be expressed:  
$$Y_j = x_{j1}\beta_1 + x_{j2}\beta_2 + x_{j3}\beta_3 + . . . + x_{jL}\beta_L + \epsilon_j$$  

with $\epsilon_j$ an independent variable identically distributed to the Normal with mean equal to zero.  
$$\epsilon_j \approx N(0,\sigma^2)_{iid}$$  

## Linear Regresion (Chap 4, Vittinghoff)  

Example of simple linear regression: exercise and glucose levels above 125 mg/dL are diagnostic of diabetes, while 100-125 mg/dL signal increased risk. Data from HERS (public data) has baseline of glucose levels among 2,032 participants in a clinical trial of Hormone Therapy (HT). Women with diabetes are excluded, to study if the exercise might help prevent progression to diabetes.  

```{r}
# ~/Dropbox/GitHub/Regression
hers <- read_csv("DataRegressBook/Chap3/hersdata.csv")

# hers data structure
hers_nodi <- filter(hers, diabetes == "no")
hers_nodi_Fit <- lm(glucose ~ exercise, data = hers_nodi)
# the linear model results can be printed using summary
summary(hers_nodi_Fit)
```

Simple linear regression model shows coefficient estimate ($\beta_1$) for exercise shows that average baseline glucose levels were about 1.7mg/dL lower among women who exercised at least three times a week than among women who exercised less.  

## For a multiple linear model  

There are models to regress several predictor variables to relate several random independent variables.  

$$y_i = E[y_i|x_i] + \epsilon_i$$  
$$Y = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \dots + \beta_p x_{p}$$  
Multiple linear regression model coefficients, the betas, give the change in $E[Y|x]$ for an increase of one unit on the predictor $x_j$ , holding other factors in the model constant; each of the estimates is adjusted for the effects of all the other predictors. As in the simple linear model the intercept $\beta_0$ (beta zero) gives the value $E[Y|x]$ when all the predictors are equal to zero. Example of multiple linear model estimate is done with:  `glucose ~ exercise + age + drinkany + BMI`.  
In general in R we can write:$Y = \beta_1 variable_1 + \beta_2 variable_2 + \beta_3 variable_3 + \beta_4 variable_4$ for a multiple linear model, in this case four regressors.  

```{r}
hers_nodi_multFit <- lm(glucose ~ exercise + age + drinkany + BMI,
                        data = hers_nodi)

# the linear model results can be printed using summary
summary(hers_nodi_multFit)
```

