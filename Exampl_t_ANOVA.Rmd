---
title: "t-Test and Anova"
author: "F.A. Barrios<br><small>Instituto de Neurobiología UNAM<br></small>"
date: "<small>`r Sys.Date()`</small>"
output:
  rmdformats::robobook:
    highlight: kate  
---
```{r setup, include=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="80")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=80)
```
  
# Examples for t-test and ANOVA

Examples of problems taken from chapter 3 (Vittinghoff's book http://www.biostat.ucsf.edu/vgsm) using data from the heart and estrogen/progestin study (HERS), a clinical trial of hormone therapy (HT) for prevention of recurrent heart attacks and death among 2,763 post-menopausal women with existing coronary heart disease (CHD). Also we will present some exercises from Daniel's book too, and this will cover a refresher for t-test and ANOVA (assuming parametric distributions).

## Introduction. 
To execute two sample comparisons the simplest is the best, as always we must apply Occam's razor. So the classical tests to estimate comparisons between tow samples are some of the most frequently-used kinds of analysis and we have the R functions:  

- Comparing two variables Fisher's F test (`var.test`)  
- Comparing two sample means with normal errors Student's t test (`t.test`)   
- Comparing two sample means with non-normal errors Wilcoxon's test (`wilcoxon.test`)  
- Comparing two proportions the binomial test (`prop.test`)  
- Correlating two variables, Pearson's or Spearman's rank correlation (`cor.test`)  
- testing for independence in contingency tables chi-squared (`chisq.test`)  
- testing small samples for correlation Fisher's exact test (`fisher.test`)

We will cover some basic tools for assessing the statistical significance of differences between the average values of a continuous outcome across two or more samples following parametric statistics are the t-Test and ANOVA.  
The example presented in (chapter 3 Table 3) of the t-Test of differences in glucose average combined with exercise for the women that are not diabetic in the HERS data set. These examples are to revisit some t-test R estimations and t-test function in R. And to remember that a **boxplot** gives a good amount of information about a numerical variable:  

- **Centering** described by the median.  
- **Dispersion**, measured by the high of the box (*interquartil* distance).  
- **Observation range** The presence of extreme values (*outliers*).  

And some information of the distribution “linked to skewness”.      
If the median (the bold line) is located toward the bottom of the box, then the data are *right-skewed* toward larger values. That is, the distance between the median and the 75th percentile is greater than that between the median and the 25th percentile. Likewise, right-skewness will be indicated if the upper whisker is longer than the lower whisker or if there are more outliers in the upper range.  

```{r}
# To load the CRAN libraries used

library(multcomp)
library(tidyverse)
library(car)
library(emmeans)
```

```{r, echo= TRUE}
HERS <- read_csv("https://raw.githubusercontent.com/fabarrios/Regression/master/DataRegressBook/Chap3/hersdata.csv", show_col_types = FALSE)
# Loading the HERS database in HERS variable
names(HERS)
summary(HERS)
boxplot(glucose ~ diabetes, data=HERS)
``` 

Before we can compare two sample means we need to test whether the sample variances are significantly different. The test statistic to compare two variances, is the Fisher's test, where you divide the larger variance by the smaller variance, if the variances are similar the ratio will be close to 1.  How do we know if the test statistic value is low or large? We look for the critical value of the Fisher's *F*.  In R there is an build in `var.test` with null hypothesis assuming the variances are the same, a large value of *p*. for Significantly different variances you can discard the null hypothesis (*p* smaller than $\alpha$)

```{r, echo= TRUE}
var.test(glucose[HERS$diabetes == "no"] ~ exercise[HERS$diabetes == "no"], 
       data=HERS, 
       alternative="two.sided")
```
The variance ratio, *F*, is close to one and the *p* value is smaller than $\alpha$ and we do not reject the null hypothesis, this variances are very close to each other and we can use the *t*-test to compare the samples means.  In R the build in *t*-test uses the null hypothesis assuming the average of the two samples to be equal, for a *p* smaller than $\alpha$ the alternative hypothesis is taken.

```{r, echo= TRUE}
# Some graphical description of the glucose state
boxplot(glucose[HERS$diabetes == "no"] ~ exercise[HERS$diabetes == "no"], data=HERS)
t.test(glucose[HERS$diabetes == "no"] ~ exercise[HERS$diabetes == "no"], data=HERS, alternative="two.sided", mu=0, var.equal=TRUE)
```
We reject the null hypothesis the glucose measurement is higher for the subjects that do not exercise, note that the confidence interval on the difference does not include zero.

## *t*-test examles
Examples of *t*-test from the exercises of the Daniel's book, Chap 7, section 3 number 3. Data can be downloaded from the WEB page of the Daniel's book.
Hoekema et al. studied the craniofacial morphology of 26 male patients with obstructive sleep apnea syndrome (OSAS) and 37 healthy male subjects (non-OSAS). One of the variables of interest was the length from the most superoanterior point of the body of the hyoid bone to the Frankfort horizontal (measured in millimeters).  

```{r, echo= TRUE}
# Daniel's chap 7 t-test examples
# EXR_C07_S03_03

Ex733 = read_csv(file="https://raw.githubusercontent.com/fabarrios/Regression/master/DataSets/ch07_all/EXR_C07_S03_03.csv", show_col_types = FALSE)
names(Ex733)
summary(Ex733)

# In parts
NoOSAS = Ex733$Length[Ex733$Group == 1]
OSAS = Ex733$Length[Ex733$Group == 2]
# 
boxplot(NoOSAS, OSAS)
var.test(OSAS, NoOSAS)
t.test(NoOSAS, OSAS, alternative="less", conf.level = 0.99)
#
# Shorter and direct
boxplot(Length ~ Group, data = Ex733)
t.test(Length ~ Group, data = Ex733, alternative = "less", conf.level = 0.99)
```

How it looks for several variables an ANOVA example using the HERS data referring to the diabetic participants.
```{r}
# Example of the HERS data for diabetic participants
# hers_yesdi <- filter(HERS, diabetes == "yes")
hers_yesdi <- HERS %>% 
  filter(diabetes == "yes") %>% 
  mutate(physact = factor(physact, levels=c("much less active","somewhat less active","about as active","somewhat more active","much more active")))

#  Example of ANOVA with HERS data for diabetic participants
#
ggplot(data = hers_yesdi, mapping = aes(x = physact, y = glucose)) + 
  geom_boxplot(na.rm = TRUE)

glucose_yesdi_activ <- aov(glucose ~ physact, data = hers_yesdi)
glucose_yesdi_activ
# Or print the anova table using the car package function
Anova(glucose_yesdi_activ, type="II")
#
S(glucose_yesdi_activ)
glucose_emmeans <- emmeans(glucose_yesdi_activ, "physact")
contrast(glucose_emmeans, adjust="sidak")
```

# Example from R-bloggers
Plotting data distributions of random data to do an example of ggplot color filling. First we build four random variables with two different distributions.  And then followed of a ggplot command using the color to distinguish the variables.  

```{r}
# Create the four groups
set.seed(10) 
df1 <- data.frame(Var="a", Value=rnorm(100,10,5))
df2 <- data.frame(Var="b", Value=rnorm(100,10,5))
df3 <- data.frame(Var="c", Value=rnorm(100,11,6))
df4 <- data.frame(Var="d", Value=rnorm(100,11,6))

# merge them in one data frame
df<-rbind(df1,df2,df3,df4)

# convert Var to a factor
df$Var <- as.factor(df$Var)
df %>% ggplot(aes(x=Value, fill=Var)) + geom_density(alpha=0.5)
```

## The ANOVA (taken from R-bloggers)
ANOVA (ANalysis Of VAriance) is a statistical test used to compare two or more groups to see if they are significantly different. The ANOVA model and some examples. The null hypothesis in ANOVA is that there is no difference between means and the alternative is that the means are not all equal. This means that when we are dealing with many groups, we cannot compare them pairwise. We can simply answer if the means between groups can be considered as equal or not.

```{r}
# ANOVA
model1 <- aov(Value ~ Var, data=df)
anova(model1)
```

## Tukey multiple comparisons
What about if we want to compare all the groups pairwise? In this case, we can apply the Tukey’s HSD which is a single-step multiple comparison procedure and statistical test, Tukey's Honest Significant Difference (Tukey's HSD). It can be used to find means that are significantly different from each other.

```{r}
summary(glht(model1, mcp(Var="Tukey")))
```
